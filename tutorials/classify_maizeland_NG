---
title: Rapid classification of croplands: Case Study of smallholder maize-cultivated lands
description: A short description of the tutorial, all on one line with no carriage returns.
author: PJNation
tags: maize, smallholder, cropland, classify, rapid, binary
date_published: 2020-06-31
---
#


<!--
Copyright 2019 The Google Earth Engine Community Authors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

#Background
Land-cover classification in complex landscapes have been constrained by inherent short-distance transition in crop/vegetation types, especially in complex smallholder farming systems. The increasing availability and accessibility of earth observation imageries provides significant opportunities to assess status and monitor changes in land cover, yet unlocking such capability is contingent on availability of relevant ground-truth data to calibrate and validate classification algorithms. The critically needed spatially-explicit ground-truth data are often unavailable in sub-Saharan African farming systems and this constrains development of relevant analytical tools to monitor cropland dynamics or generate [near]real-time insights on farming systems.  This tutorial was developed as a quick guide for users who are interested in implementing landcover classification routine in google earth engine environment, using ground-truth data and available sentinel 2A spectral bands imageries. The goal is to provide an easy-to-implement workflow that can be adapted by researchers and analysts to quickly classify croplands. As more efforts are invested in collecting spatially-rich georeferenced data at national and regional levels, this tutorial can be tweaked to generate immeditiate/timely insights.

#Caveat
This landcover classification was implemented based on available data as collected under a  multi-year project (https://tamasa.cimmyt.org/) which was focused on advancing digital agronomic innovation for decision support in maize-based farming systems. Therefore, the available ground-truth data is rich in maize farm locations, and contains much fewer datapoints for other croptypes within the focal geography. Cosidering this limitation, the scope of this classification tool and this tutorial is limited to binary classification of maizelands (i.e. maize vs. non-maize cultivated) within the period of data collection (i.e. 2017).


##Outline

1. Ingesting, Importing, and Visualizing Data
a.  Ingest boundary files
b.  Ingest groundtruth data
c.  Import sentinel 2A Imageries

2.	Setting-Up and Implementing Analytics
a. Select Bands and extract training data
b. Train Classifiers, calculate error matrix, and assess training accuracy
c. Test Classification outputs, generate validation matrix, and calculate accuracy

3.	Export Outputs

4.	Final notes


##Ingesting, Importing, and Visualizing Data

a. Ingest the Nigerian boundary as the focal geography and maize target region boundary as the area of interest (AOI). Using the code below, you will import a FeatureCollection object, and filter by "Country" to select "Nigeria". FeatureCollections are groups of features (spatial data and attributes). Filter is the method to extract a specific set of features from a feature collection. Assign the output to a variable called "nigerianBorder". The imagery analyses will be limited to the maize target region in Nigeria, i.e. the region that accounts for ~70% of Nigeria's maize production. Therefore, you will import a predefined shapefile layer (already converted to GEE asset) and assign to the variable "aoi". The maize target region asset can be assessed here (https://code.earthengine.google.com/?asset=users/juliusadewopo/MzeTargetRegion_alt_dslv2).  Display both layers to the map it using Map.addLayer() and tweak the symbology with the color parameters specified below.

// Ingest country boundaries feature collection.
var dataset = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017');

// Apply filter where country name equals Nigeria.
var nigeriaBorder = dataset.filter(ee.Filter.eq('country_na', 'Nigeria'));

// Print new "nigeriaBorder" object and explore features and properties.
// There should only be one feature representing Nigeria.
print(nigeriaBorder);

// Center the map view on Nigeria, otherwise, the view defaults to the Country of your IP address
Map.centerObject(nigeriaBorder, 6);

//Setting color parameters for Nigeria Boundary
var shown = true; // true or false, 1 or 0 
var opacity = 0.2; // number [0-1]
var nameLayer = 'map'; // string
var visParams = {color: 'red'}; // dictionary: 

//Add Nigeria as a layer to the map view
Map.addLayer(nigeriaBorder, visParams, nameLayer, shown, opacity);

//Setting color parameters for Maize target area boundary
var shown = true; // true or false, 1 or 0 
var opacity = 0.5; // number [0-1]
var nameLayer = 'map2'; // string
var visParams = {color: 'brown', strokeWidth: 5}; // dictionary:
Map.addLayer(aoi, visParams, nameLayer, shown, opacity);


b. Ingest groundtruth data for georeferenced locations where maize (and other crops) were cultivated during the growing season of 2017 (June - Oct). The data has been pre-processed and randomly split (70:30) into training and testing datasets. Import the training dataset as GEE asset here (https://code.earthengine.google.com/?asset=users/juliusadewopo/Mzetrgt_Train17_Rev), and the testing dataset here (https://code.earthengine.google.com/?asset=users/juliusadewopo/Mzetrgt_Test17_Rev). Assing variable names "trainpts" and "testpts" to the training and testing points, respectively, and add them as layer to the map view.

//Display training and test points to visualize distribution within the aoi
Map.addLayer(trainpts, {color:'FF0000'});
Map.addLayer(testpts, {color:'00FFFF'});


c. Import sentinel 2A Imageries

Next, you will import Copernicus Sentinel 2A spectral band imageries. The imageries are organized as an ImageCollection object, which is a container for a collection of individual images. With the code snippet below, you will import the sentinel 2A ImageCollection , and similar  method can be used to import an ImageCollection for other types of multi-temporal or multi-spectral data including Landsat, vegetation index,  rainfall, temperature etc. Considering the context, you will apply relevant filters to restrict selected imagery tiles to the aoi and date range for the growing season in 2017 (to coincide with the period of data collection). Standard quality control bands for cloud will be used in a function to create a mask layer, ans assigned to variable "maskS2Clouds". The function is passed on to a variable that minimizes cloud contamination by selecting pixels with the least percentage of cloud contamination within the temporal imagery composite (during the season). which will be combined with other filters to generate a "mosaic" of cloud-minimized imageries. Note that the aoi is north of the equator and often characterized by heavy cloudiness during the rainy season, so this workflow is essesntial and can be further tweaked to achieve better results.

//Ingest sentinel 2A imageries
var s2 = ee.ImageCollection("COPERNICUS/S2");

// Define parameters for cloudmask; Bits 10 and 11 are clouds and cirrus, respectively.
var cloudBitMask = ee.Number(2).pow(10).int();
var cirrusBitMask = ee.Number(2).pow(11).int();

//set function to generate cloud mask
function maskS2clouds(image) {
  var qa = image.select('QA60');
  // Both flags should be set to zero, indicating clear conditions.
  var mask = qa.bitwiseAnd(cloudBitMask).eq(0).and(
             qa.bitwiseAnd(cirrusBitMask).eq(0));
  return image.updateMask(mask);
}

//Apply the cloud mask with other filters to derive a mosaic within spatial and temporal context
var cloudMasked = s2.filterBounds(aoi).map(maskS2clouds).filterDate('2017-06-15', '2017-10-15');
var min = cloudMasked.min();
var mosaic = ee.ImageCollection(min).mosaic();

//Create Custom mosaic from selected bands to visualize the cloud-minimized imagery; You apply similar code to compare the initial cloud-contaminated imagery, setting "s2" as the image
Map.addLayer(mosaic, {bands: ['B4', 'B3', 'B2'], max: 2000}, 'custom mosaic');


2.	Setting-Up and Implementing Analytics
a. Now that you have prepared the mosaic, proceed to select that spectral bands that are relevant for the classification. The more bands selected, the more computationally intensive the classification will be. In the code below, all bands of the S2A are selected, but you can tweak this by selecting fewer bands. Note that our goal is to utilize as much spectral information as possible to train the classifier algorithm to differentiate between maize and non-maize. It's probable that generating additional training points from the imagery may help to further achieve this objective. The training points (trainpts) will be used to extract the reflectance values of the pixels from all spectral bands and this will be passed to the classifier algorithms.

//Specify and select bands that will be used in the classification
var bands = ['B1','B2','B3','B4','B5','B6', 'B7', 'B8', 'B8A', 'B9', 'B10','B11', 'B12'];

var image_cl = mosaic
  .select(bands);

// Overlay the points on the imagery to get training.
var training = image_cl.sampleRegions({
  collection: trainpts,
  properties: ['class'],
  scale: 30
});


b. For the binary classification you will be applying 2 classifiers - classification and regression trees (CART) and Random Forest (RF), which are both suitable for categorical classification and have been used in various contexts for classification. By comparing outputs from both CART and RF, users can make objective inference on most accurate classifier. Default parameters will be accepted and further tweaks (such as optiming number of trees in RF) is outside the scope of this tutorial. The output imagery will be a bi-colored imagery, and the console will show the metrics. While reviewing the output metrics, note that maize is labeled as "0" while non-maize is labeled as "1".

// Train a CART classifier with default parameters.
var trained = ee.Classifier.smileCart().train(training, 'class', bands);

//Train a RF classifier with default parameters.
var trained_rf = ee.Classifier.smileRandomForest(10)
    .train({
      features: training,
      classProperty: 'class',
      inputProperties: bands
    });

// Classify the image with the same bands used for training.
var classified = image_cl.select(bands).classify(trained);
var classified_rf = image_cl.select(bands).classify(trained_rf);

// Create a palette to display the classes.
var palette =['00008B', '32CD32'];

//Add the output of the training classification to the map view
Map.addLayer(classified,{min: 0, max: 1, palette: palette},'class');
Map.addLayer(classified_rf,{min: 0, max: 1, palette: palette},'class');


// Calculate the training error matrix and accuracy for both classifiers by using the "confusionMatrix" function to generate metrics on the resubstitution accuracy, as shown below

//Accuracy calculation for CART
var trainAccuracy = trained.confusionMatrix();
print('Resubstitution error matrix: ', trainAccuracy);
print('Training overall accuracy: ', trainAccuracy.accuracy());

//Accuracy calculation for RF
var trainAccuracy_rf = trained_rf.confusionMatrix();
print('Resubstitution error matrix: ', trainAccuracy_rf);
print('Training overall accuracy: ', trainAccuracy_rf.accuracy());
  
  
c. To assess the reliability of the classification outputs, use the "testpts" dataset (earlier ingested as assets) to extract spectral information from the bands. You will further apply ee.Filter.neq on the "B1" band to remove pixels with null value, and extract the classified values for the "testpts" pixels from the training-mode algorithm classification. Note that different accuracy assessment is conducted for each classifier.
    
  // Use the testpts to extract pixel values from the bands for validation
 var testing = image_cl.sampleRegions({
  collection: testpts,
  properties: ['class'],
  scale: 30
}).filter(ee.Filter.neq('B1', null)); //filter added to rid out null pixels

// Classify the validation data
var validated = testing.classify(trained);
var validated_rf = testing.classify(trained_rf);

// Calculate confusion matrix to generate validation accuracy for CART
var testAccuracy = validated.errorMatrix('class', 'classification');
print('Validation error matrix: ', testAccuracy);
print('Validation overall accuracy: ', testAccuracy.accuracy());

// Calculate confusion matrix to generate validation accuracy for CART
var testAccuracy_rf = validated_rf.errorMatrix('class', 'classification');
print('Validation error matrix: ', testAccuracy_rf);
print('Validation overall accuracy: ', testAccuracy_rf.accuracy());

//Add the "aoi" layer on top of the classified grid for visualization. You can tweak the opacity parameter or turn off the "aoi" layer to see the layer beneath more clearly
Map.addLayer(aoi);   


## Section heading 1

Break up your tutorial into manageable sections.

With one or more paragraphs, separated by a blank line.

Inside your sections, you can also:

1. Use numbered lists
1. ..when the order..
1. ..of items is important.

And:

- This is a bulleted list.
- Use bulleted lists when items are not strictly ordered.

..and even:

Use     | tables   | to organize | content
------- | -------- | ----------- | -------
Your    | tables   | can         | also
contain | multiple | rows        | ...

## Section heading 2

Use separate sections for related, but discrete, groups of steps.

Use code blocks to show users how to do something after describing it:

```
// Use comments to describe details that can't be easily expressed in code.
// Always try making code more self descriptive before adding a comment.
// Similarly, avoid repeating verbatim what's already said in code
// (e.g., "assign ImageCollection to variable 'coll'").
var coll = ee.ImageCollection('LANDSAT/LC08/C01/T1_TOA');
```

### Use subsections if appropriate

Consider breaking longer sections that cover multiple topics or span multiple
pages into subsections.
